{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras import layers\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import annutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfin_on = pd.read_csv('smscg_input_on.csv',index_col=0,parse_dates=True)\n",
    "dfin_off = pd.read_csv('smscg_input_off.csv',index_col=0,parse_dates=True)\n",
    "\n",
    "dfout_on = pd.read_csv('smscg_output_on.csv',index_col=0,parse_dates=True)\n",
    "dfout_off = pd.read_csv('smscg_output_off.csv',index_col=0,parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create functions to load and interact with ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import annutils\n",
    "import joblib\n",
    "class ANNModel:\n",
    "    '''\n",
    "    model consists of the model file + the scaling of inputs and outputs\n",
    "    '''\n",
    "    def __init__(self,model,xscaler,yscaler):\n",
    "        self.model=model\n",
    "        self.xscaler=xscaler\n",
    "        self.yscaler=yscaler\n",
    "    def predict(self, dfin):\n",
    "        return annutils.predict(self.model,dfin,self.xscaler,self.yscaler)\n",
    "def load_model(location):\n",
    "    '''\n",
    "    load location i.e. RSAN018\n",
    "    '''\n",
    "    model=keras.models.load_model('%s_EC_ff_8x2.h5'%location)\n",
    "    xscaler,yscaler=joblib.load('%s_EC-xyscaler.dump'%location)\n",
    "    return ANNModel(model,xscaler,yscaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with loaded anns and scaler the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('RSAN018')\n",
    "dfp=model.predict(dfin_on)\n",
    "pd.concat([dfout_on['RSAN018_EC'],dfp],axis=1).hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For example increasing the exports by 100 cfs ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfin_exp_plus100=dfin_on.copy()\n",
    "dfin_exp_plus100.loc[:,'exports']+=100\n",
    "dfin_exp_plus100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ... what is the SAC flow adjustment needed to get the same predicted values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpexp=model.predict(dfin_exp_plus100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp.hvplot(label='original')*dfpexp.hvplot(label='+100 exports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the cost function\n",
    "The first step to optimizing is to define a cost function which will be \"minimized\" by scipy.optimize\n",
    "\n",
    "In this case the cost function takes an array of sac_flow_adjustment which is is added to the 'sac' column of the input. The sac_flow_adjustment is the same size as the dfin. \n",
    "\n",
    "Note: Be careful because the size of the problem i.e. the size of dfin can cause out of memory issues and can make the problem too hard to do on a single processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy.optimize as optimize\n",
    "def adjust_input(input_change,input_col,dfin):\n",
    "    '''\n",
    "    adjust input by adding the input_change array to the input_col of the pandas data frame dfin\n",
    "    return a copy of the changed array \n",
    "    \n",
    "    dfin is assumed to be the same size as input_change\n",
    "    '''\n",
    "    dfinc=dfin.copy()\n",
    "    dfinc.loc[:,input_col]+=input_change\n",
    "    return dfinc\n",
    "\n",
    "def adjust_predict(model, input_change, input_col, dfin):\n",
    "    '''\n",
    "    adjust the dfin[:, input_col] by adding input_change\n",
    "    returns prediction with the model \n",
    "    '''\n",
    "    dfp=model.predict(adjust_input(input_change,input_col,dfin))\n",
    "    return dfp\n",
    "\n",
    "def cost_func(sac_flow_adjustment,model,dfin,dftarget):\n",
    "    '''\n",
    "    Calculate the SSE between the dforiginal and the model prediction using dfin as input. \n",
    "    The goal is to minimize this using scipy.optimize(cost_func)\n",
    "    '''\n",
    "    dfp=adjust_predict(model,sac_flow_adjustment,'sac',dfin)\n",
    "    x,y=annutils.synchronize(dftarget,dfp)\n",
    "    return mean_squared_error(x,y)# penalty ? +np.sum(np.diff(sac_flow_adjustment)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on a time period, e.g. 1990-09 to 1990-10\n",
    "Choose a time window and then create an inputs data set that includes 117 days, i.e. the antecedent memory of the system\n",
    "Then choose the initial value xvar ( represents additional flow required in 'sac' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twslice=slice('1990-09-01','1990-09-30')\n",
    "var_start=pd.to_datetime(twslice.start)-pd.Timedelta(117,'D')\n",
    "var_end=pd.to_datetime(twslice.stop)\n",
    "# increase exports by 100 only for time slice\n",
    "dfx0=dfin_on.copy()\n",
    "dfx0=dfx0.loc[var_start:var_end,:]\n",
    "dfx0.loc[twslice,'exports']+=100\n",
    "#initial guess same as amount of exports (100) above (array size is +1 as end is included)\n",
    "xvar=np.zeros((var_end-var_start).days+1)\n",
    "xvar[-30:]=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx0.loc[twslice,['sac','exports']].hvplot()*dfin_on.loc[twslice,['sac','exports']].hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose SLSQP optimizer for the cost function\n",
    "The cost function is the SSE w.r.t to the EC before the exports were increased. This should give us the additional 'sac' flow needed to minimize the impact of additional exports. In the Sacramento San Joaquin Delta this is can be used to calculated \"carriage\" water i.e. the additional water above exports increase (in this case 100) to have no impact to the water quality conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xguess=xvar\n",
    "opt_result_slsqp=optimize.minimize(cost_func,xguess,(model,dfx0,dfp),method='SLSQP',\n",
    "                              options={'ftol': 1e-2, 'eps':1, 'maxiter':50})#,callback=optimize_callback)\n",
    "opt_result_slsqp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfans=adjust_input(opt_result_slsqp.x,'sac',dfx0)\n",
    "dfin_on.loc[var_start:var_end,['sac','exports']].hvplot(label='base')\\\n",
    " *dfx0.loc[var_start:var_end,['sac','exports']].hvplot(label='exports+100')\\\n",
    " *dfans.loc[var_start:var_end,['sac','exports']].hvplot(label='sac adjusted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(opt_result_slsqp.x,dfx0.index).hvplot(label='Additional Sacramento Flow (cfs) required to offset impact of +100cfs exports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(opt_result_slsqp.x,dfx0.index)-100)[twslice].hvplot(label='Carriage water for 100 cfs exports increase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfadj=adjust_predict(model,opt_result_slsqp.x,'sac',dfx0)\n",
    "dfexp=adjust_predict(model,0,'sac',dfx0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfadj.hvplot(label='sac adjusted')*dfp.loc[dfadj.index].hvplot(label='base')*dfexp.loc[dfadj.index].hvplot(label='exp+100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.concat([model.predict(dfin_on),model.predict(dfx0)],axis=1)\n",
    "df1.columns=['base','exports']\n",
    "df1[var_start:var_end].hvplot(title='EC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_result_cobyla=optimize.minimize(cost_func,xvar,(model,dfx0,dfp),method='cobyla',options={'maxiter':1000,'tol':1e-2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_result_cobyla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(opt_result_slsqp.x,dfx0.index).hvplot(label='slsqp')*pd.DataFrame(opt_result_cobyla.x,dfx0.index).hvplot(label='cobyla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt_result_bfgs=optimize.minimize(cost_func,xvar,(model,dfx0,dfp),method='bfgs',options={'eps':1, 'gtol':1, 'maxiter':50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
