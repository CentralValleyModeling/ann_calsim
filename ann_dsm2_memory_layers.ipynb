{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33e7463",
   "metadata": {},
   "source": [
    "# Train ANN with DSM2 data\n",
    "\n",
    "This notebook uses a memory input with CNN, GRU and desnse layer(s) to get a fit for the DSM2 produced data. Again the aim is to show the effectiveness of all of the above layers as potential ANN model layers that show decent performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42880e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras import layers\n",
    "#import keras\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import hvplot.pandas\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452f51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__, np.__version__, pd.__version__, pn.__version__, hv.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import annutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db640526",
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist = [pd.read_excel('./dsm2_ann_BaselineData_20220120.xlsx',i,index_col=0,parse_dates=True) for i in range(8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfinps = pd.concat(dflist[0:7],axis=1)\n",
    "dfinps.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfouts = dflist[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b598dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfouts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84a39bc",
   "metadata": {},
   "source": [
    "## Tensorflow Board Setup\n",
    "A log directory to keep the training logs\n",
    "\n",
    "Tensorboard starts a separate process and is best started from the command line. Open a command window and activate this environment (i.e. keras) and goto the current directory. Then type in\n",
    "```\n",
    "tensorboard --logdir=./tf_training_logs/ --port=6006\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb97dc84-0cd9-4d00-87f8-952abbf2efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe61039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard\n",
    "#%tensorboard --logdir=./tf_training_logs/ --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fd54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"tf_training_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a209223b",
   "metadata": {},
   "source": [
    "# Calibration and Validation Periods\n",
    "Calibration is from 1940 - 2015 and Validation from 1923 - 1939 as per the Calsim 3 ANN paper\n",
    "\n",
    "The output locations are names of the columns in the output(labels) csv files. For each location, an ANN is trained on all the specified data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b15b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_locations = list(dfouts.columns)\n",
    "calib_slice = slice('1990', '2020')\n",
    "valid_slice = slice('1990', '2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3db9905",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfinps, dfouts = annutils.synchronize(dfinps, dfouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f016b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs,ys = annutils.create_xyscaler(dfinps, dfouts.iloc[:,[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3408a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = xs.transform(dfinps) # time x ninputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6912c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = ys.transform(dfouts.iloc[:,[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3a7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt,Yt=annutils.create_memory_sequence_set(xtrain, ytrain, time_memory=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a3749a",
   "metadata": {},
   "source": [
    "xx and yy are input and target time series synchronized on the same time \n",
    "\n",
    "\n",
    "This is has been transformed to a Xt and Yt\n",
    " * Xt is an array of number of batches x time memory x number of features\n",
    " * Yt is an array of the output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e06dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt.shape,Yt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b49672",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803eca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sequential model with 3 layers\n",
    "NFEATURES = 7  # 126  # (8 + 10)*7\n",
    "\n",
    "\n",
    "def build_dense_model():\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=[120, NFEATURES]),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(4, activation='sigmoid'),\n",
    "        keras.layers.Dense(2, activation='sigmoid'),\n",
    "        keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=0.001), loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_layer_from_string_def(s='i120'):\n",
    "    if s[0:3] == 'c1d':\n",
    "        fields = s[3:].split('x')\n",
    "        return keras.layers.Conv1D(filters=int(fields[0]), kernel_size=int(fields[1]), strides=int(fields[2]),\n",
    "                                   padding='causal', activation='linear')\n",
    "    elif s[0:2] == 'td':\n",
    "        return keras.layers.TimeDistributed(keras.layers.Dense(int(s[2:]), activation='elu'))\n",
    "    elif s[0:2] == 'dr':\n",
    "        return keras.layers.Dropout(float(s[2:]))\n",
    "    elif s[0] == 'i':\n",
    "        return keras.layers.InputLayer(input_shape=[int(s[1:]), NFEATURES])\n",
    "    elif s[0] == 'f':\n",
    "        return keras.layers.Flatten()\n",
    "    elif s[0] == 'g':\n",
    "        return keras.layers.GRU(int(s[1:]), return_sequences=True, activation='relu')\n",
    "    elif s[0] == 'd':\n",
    "        return keras.layers.Dense(int(s[1:]), activation='elu')\n",
    "    elif s[0] == 'o':\n",
    "        return keras.layers.Dense(int(s[1:]), activation='linear')\n",
    "    else:\n",
    "        raise Exception('Unknown layer def: %s' % s)\n",
    "\n",
    "\n",
    "def build_model_from_string_def(strdef='i120_f_d4_d2_d1'):\n",
    "    model = keras.models.Sequential(\n",
    "        [build_layer_from_string_def(f) for f in strdef.split('_')])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=0.001), loss=\"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08342e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow Board Setup\n",
    "#model_str_def = \"i120_c1d5x1x1_c1d5x4x1_g4_g2_f_d4_d2_o1\"\n",
    "#model_str_def = \"i120_f_d4_d2_o1\"\n",
    "#model_str_def = \"i120_c1d15x1x1_c1d15x4x1_td4_td2_o1\"\n",
    "#model_str_def = \"i120_g4_g2_f_o1\"\n",
    "model_str_def = 'i120_c1d10x3x1_f_d4_d2_o1'\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(os.path.join(root_logdir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "                                                          model_str_def))\n",
    "# check if model has been run before\n",
    "try:\n",
    "    model = annutils.load_model(model_str_def).model\n",
    "    #assuming that loaded model xs,ys is the same: FIXME:\n",
    "    print('Using existing model from files for %s'%model_str_def)\n",
    "except:\n",
    "    print('Creating New Model: ')\n",
    "    model = build_model_from_string_def(model_str_def)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395ffd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "model.fit(Xt, Yt, epochs=1000, batch_size=120, \n",
    "#          validation_data=(Xt[split_point+365:],Yt[split_point+365:]),\n",
    "          callbacks=[\n",
    "              keras.callbacks.EarlyStopping(\n",
    "                  monitor=\"loss\", patience=25, mode=\"min\", restore_best_weights=True),\n",
    "              tensorboard_cb\n",
    "          ],\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bace1e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "annutils.save_model(model_str_def, model, xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015a35b4",
   "metadata": {},
   "source": [
    "# Show the performance on the data sets visually\n",
    "\n",
    "Change the location to one of the locations for which the ANN is trained and run cells below to see performance on one or more of the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytp = model.predict(Xt)[:,0].flatten()\n",
    "\n",
    "dfp = pd.concat([pd.DataFrame(y,index=dfouts.index[120:]) for y in [Yt, Ytp]],axis=1)\n",
    "dfp.columns=['actual','predicted']\n",
    "\n",
    "dfp.hvplot(width=800,legend='top_right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166a544",
   "metadata": {},
   "source": [
    "# Display weights and x and y scaling parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972dcaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "annmodel = annutils.load_model(model_str_def)\n",
    "\n",
    "annmodel.model.get_weights()\n",
    "annmodel.xscaler.data_min_, annmodel.xscaler.data_max_\n",
    "annmodel.xscaler.feature_range\n",
    "annmodel.xscaler.min_\n",
    "annmodel.xscaler.scale_\n",
    "\n",
    "print('Ann model loaded for %s'%model_str_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ac231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anntraining-numpy1.19]",
   "language": "python",
   "name": "conda-env-anntraining-numpy1.19-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
